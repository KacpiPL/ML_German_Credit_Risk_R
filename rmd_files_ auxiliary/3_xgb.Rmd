---
title: "ML - German_Credit_Risk"
author: "Bryzik Micha≈Ç, Gruca Kacper"
date: "2024-01-09"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
  html_notebook:
    toc: true
    toc_depth: '4'
---

```{r, message=FALSE}
library(dplyr)
library(rpart)
library(rpart.plot)
library(xgboost)
library(plyr)
library(doParallel)
library(pROC)
library(ggplot2)
library(caret)

cores=detectCores()
registerDoParallel(cores=cores)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
options(width = 60)
local({
  hook_output <- knitr::knit_hooks$get('output')
  knitr::knit_hooks$set(output = function(x, options) {
    if (!is.null(options$max.height)) options$attr.output <- c(
      options$attr.output,
      sprintf('style="max-height: %s;"', options$max.height)
    )
    hook_output(x, options)
  })
})
```

## XGB - one by one

In this section we apply the approach from classes adjusted to our case. We build the xgb model by hypertuning parameters one by one in the following order:

* nrounds,
* max_depth and min_child_weight,
* colsample_bytree,
* subsample,
* change learning rate and number of trees,
* change learning rate and number of trees again.

Let's start with cleaning the environment and loading the data.

```{r}
rm(list=ls())

df <- read.csv("./data/output/df.csv")
df.train <- read.csv("./data/output/df.train.csv")
df.test <- read.csv("./data/output/df.test.csv")
```

Xgb requires target variable as factor - let's change it

```{r}
df$class <- factor(df$class, levels = c(0, 1), labels = c("bad", "good"))
df.train$class <- factor(df.train$class, levels = c(0, 1), labels = c("bad", "good"))
df.test$class <- factor(df.test$class, levels = c(0, 1), labels = c("bad", "good"))
```

Take columns chosen by RFE

```{r}
# Read columns chosen by rfe
rfe_columns <- read.csv("./data/output/rfe_columns.csv")

# Add "class" to the list of columns chosen for analysis
rfe_columns <- c("class", rfe_columns$x)

# Modify dfs to have columns chosen by rfe
df.train <- df.train[, rfe_columns]
df.test <- df.test[, rfe_columns]
```

Calculate initial parameters based on recommendations provided during the lectures.

```{r}
# colsample_bytree - # rule of thumb (number of predictors)
sqrt(35)/35.  # 0.1690309

# min_child_weight - 0.5 ... 0.1% of number of observations
0.005 * 2000  # 10
0.01 * 2000   # 20
```


### Hyperparameter tuning

#### Nrounds

```{r, warning=FALSE}
set.seed(12345678)
xgb_grid <- expand.grid(nrounds = seq(20, 80, 10),
                              max_depth = c(8),
                              eta = c(0.25),
                              gamma = 1,
                              colsample_bytree = c(0.17),
                              min_child_weight = c(10),
                              subsample = 0.8)

train_control <- trainControl(method = "cv", 
                              number = 5,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)

xgb_model.1 <- train(class ~ .,
                   data = df.train,
                   method = "xgbTree",
                   trControl = train_control,
                   tuneGrid  = xgb_grid)
```

```{r, max.height='150px'}
xgb_model.1
```
```{r}
xgb_model.1 %>% saveRDS("./data/output/xgb/xgb_model.1.rds")
```

The best for nrounds = 40
Let's take this parameter and tuner others.

#### Max_depth and min_child_weight

```{r, message = FALSE}
xgb_grid <- expand.grid(nrounds = 40,
                        max_depth = c(5, 15, 1),
                        eta = c(0.25),
                        gamma = 1,
                        colsample_bytree = c(0.17),
                        min_child_weight = seq(10, 200, 2),
                        subsample = 0.8)
set.seed(12345678)
xgb_model.2 <- train(class ~ .,
                     data = df.train,
                     method = "xgbTree",
                     trControl = train_control,
                     tuneGrid  = xgb_grid)
```

```{r, max.height='150px'}
xgb_model.2
```

```{r}
xgb_model.2 %>% saveRDS("./data/output/xgb/xgb_model.2.rds")
```

The best for max_depth = 5, min_child_weight = 12.
Let's take this parameter and tuner others.

#### Colsample_bytree

```{r}
xgb_grid <- expand.grid(nrounds = 40,
                        max_depth = 5,
                        eta = c(0.25),
                        gamma = 1,
                        colsample_bytree = seq(0.05, 0.7, 0.01),
                        min_child_weight = 12,
                        subsample = 0.8)

set.seed(12345678)
xgb_model.3 <- train(class ~ .,
                     data = df.train,
                     method = "xgbTree",
                     trControl = train_control,
                     tuneGrid  = xgb_grid)
```

```{r, max.height='150px'}
xgb_model.3
```

```{r}
xgb_model.3 %>% saveRDS("./data/output/xgb/xgb_model.3.rds")
```

The best for colsample_bytree = 0.65.
Let's take this parameter and tuner others.

#### Subsample

```{r, message=FALSE}
xgb_grid <- expand.grid(nrounds = 40,
                        max_depth = 5,
                        eta = c(0.25),
                        gamma = 1,
                        colsample_bytree = 0.65,
                        min_child_weight = 12,
                        subsample = seq(0.6, 0.9, 0.05))

set.seed(12345678)
xgb_model.4 <- train(class ~ .,
                     data = df.train,
                     method = "xgbTree",
                     trControl = train_control,
                     tuneGrid  = xgb_grid)
```

```{r, max.height='150px'}
xgb_model.4
```

```{r}
xgb_model.4 %>% saveRDS("./data/output/xgb/xgb_model.4.rds")
```

The best for subsample = 0.85.
Let's take this parameter and tuner others.

#### Learning rate and number of trees

Let's double number of trees and reduce by half learning rate

```{r}
# Change learning rate and number of trees
xgb_grid <- expand.grid(nrounds = 80,
                        max_depth = 5,
                        eta = c(0.12),
                        gamma = 1,
                        colsample_bytree = 0.65,
                        min_child_weight = 12,
                        subsample = 0.85)

set.seed(12345678)
xgb_model.5 <- train(class ~ .,
                     data = df.train,
                     method = "xgbTree",
                     trControl = train_control,
                     tuneGrid  = xgb_grid)
```

```{r}
xgb_model.5
```

```{r}
xgb_model.5 %>% saveRDS("./data/output/xgb/xgb_model.5.rds")
```

#### Learning rate and number of trees

Let's double number of trees and reduce by half learning rate again

```{r}
# Change learning rate and number of trees
xgb_grid <- expand.grid(nrounds = 160,
                        max_depth = 5,
                        eta = c(0.06),
                        gamma = 1,
                        colsample_bytree = 0.65,
                        min_child_weight = 12,
                        subsample = 0.85)

set.seed(12345678)
xgb_model.6 <- train(class ~ .,
                     data = df.train,
                     method = "xgbTree",
                     trControl = train_control,
                     tuneGrid  = xgb_grid)
```

```{r}
xgb_model.6
```

```{r}
xgb_model.6 %>% saveRDS("./data/output/xgb/xgb_model.6.rds")
```

Compare models on test dataframe

```{r}
# Load function to take accuracy and gini
source("./scripts/getAccuracyAndGini.R")

models <- c("1":"6")

# Apply the function to every model and build table
sapply(paste0("xgb_model.", models),
       function(x) getAccuracyAndGini(model = get(x),
                                      data = df.test,
                                      target_variable = "class",
                                      predicted_class = "good")
)
```

xgb_model.5 has the highest values, so we choose this one as final for xgb.

#### Calculate ROC

```{r, message=FALSE}
ROC.train <- pROC::roc(df.train$class, 
                       predict(xgb_model.6,
                               df.train, type = "prob")[, "good"])

ROC.test  <- pROC::roc(df.test$class, 
                       predict(xgb_model.6,
                               df.test, type = "prob")[, "good"])

cat("AUC for train = ", pROC::auc(ROC.train), 
    ", Gini for train = ", 2 * pROC::auc(ROC.train) - 1, "\n", sep = "")

cat("AUC for test = ", pROC::auc(ROC.test), 
    ", Gini for test = ", 2 * pROC::auc(ROC.test) - 1, "\n", sep = "")

```

#### Chart of xgb ROC

```{r}
list(
  ROC.train   = ROC.train,
  ROC.test    = ROC.test
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(subtitle = paste0("Gini TRAIN: ",
                         "xgb = ", 
                         round(100 * (2 * auc(ROC.train) - 1), 1), "%, ",
                         "Gini TEST: ",
                         "xgb = ", 
                         round(100 * (2 * auc(ROC.test) - 1), 1), "%, "
                         )) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired")
```











