---
title: "ML - German_Credit_Risk"
author: "Bryzik Michał, Gruca Kacper"
date: "2024-03-04"
output: 
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE}
options(width = 60)
local({
  hook_output <- knitr::knit_hooks$get('output')
  knitr::knit_hooks$set(output = function(x, options) {
    if (!is.null(options$max.height)) options$attr.output <- c(
      options$attr.output,
      sprintf('style="max-height: %s;"', options$max.height)
    )
    hook_output(x, options)
  })
})
```

## Introduction

Classification problem

The main goal of this paper is to apply ML methods to classify clients as "good" or "bad".

We base on the dataset delivered by our lecturer. The dataset contains additional 10 "mysterious" variables (feat01 - feat10) generated by him. While the exact details of how these variables were generated are not provided, they may contain additional features or information that could be relevant for analysis.

As the following sections are independent of each other at the beginning of almost every section, we clean the environment and load the necessary data and libraries.

First of all we will clear the data, then perform simple Exploratory Data Analysis. Later on we will choose the final features using RFE and we will move to building models: XGB, GBM and RF using the approach from classes (hypertuning parameters one by one) and then try to build models using tidymodels package and some general approach.

```{r, message=FALSE}
library(tidyverse)
library(DT)
library(dplyr)
library(fastDummies)
library(caret)
library(ggplot2)
library(plotfunctions)
```

## Data cleaning

Read csv
```{r}
df <- read.csv("data/input/c2.csv")
```

Below we can observe raw dataframe and its format of the columns:

```{r, echo=FALSE}
datatable(df, options = list(
  searching = FALSE,
  pageLength = 5,
  lengthMenu = c(5, 10, 15, 20),
  scrollX = TRUE
), width = "100%")  # Adjust the width as needed
```

```{r, max.height='150px'}
str(df)
```

### Delete quotations

As we can observe all chr columns have additiona quotes '', let's delete them.

```{r}
df <- df %>%
  mutate_if(is.character, function(x) gsub("'", "", x))
```

### Missing values

Dataframe does not have any missing values, as we can see below

```{r}
## Getting the number of missing values in each column
num_missing <-  colSums(is.na(df))

# Excluding columns that contains 0 missing values
num_missing <-  num_missing[num_missing > 0]
num_missing

```

### Handle Categorical variables

Select all columns with chr data type - we treat them as categorical

```{r}
chr_columns <- names(df[, sapply(df, is.character)])
chr_columns
```

Create df with categorical variables, print unique values of each column.

```{r}
# Function to print unique values for each column in dataframe
print_unique_values <- function(df){
  
  df_unique <- lapply(df, unique)
  # Print unique values for each column
  for (col_name in names(df_unique)) {
    cat(col_name, ": ", paste(df_unique[[col_name]], collapse = ", "), "\n")
  }
}

```

```{r}
df_categorical <- df[chr_columns]

print_unique_values(df_categorical)
       
```

#### Personal status column

Let's focus on the column "personal_status", here we have 2 information, sex and marital status

We should separate them into 2 columns:

* sex (0 - female, 1 - male)
* marital_status:
  + single
  + div/dep/mar
  + mar/wid
  + div/sep

Check the size of each group:

```{r}
personal_status_counts <- table(df$personal_status)

print(personal_status_counts)
```

Create 2 new columns:

```{r}
# Extracting sex information
df <- df %>%
  mutate(sex = ifelse(grepl("female", personal_status), 0, 1))

# Extracting the rest of the information
df <- df %>%
  mutate(marital_status = sub("^[^ ]+ ", "", sub(" $", "", personal_status)))

# Drop the original "personal_status" column
df <- df %>%
  select(-personal_status)
```

```{r}
sex_counts <- table(df$sex)
marital_status_counts <- table(df$marital_status)

print(sex_counts)
print(marital_status_counts)
```

Create a list of reamining chr columns which needs to be handled yet

```{r}
# Copy chr columns and remove personal_status - now we have list with remaining chr columns
chr_columns_help <- chr_columns

# Remove "personal_status" from list
chr_columns_help <- setdiff(chr_columns_help, "personal_status")

# Adding marital_status to object_columns_help
chr_columns_help <- union(chr_columns_help, "marital_status")

print(chr_columns_help)
```

#### Ordered categorical variables

In the dataset we can observe a few ordered categorical variables. Let's define and transform them.

```{r}
# Refresh the df_categorical and show unique values - columns to be handled yet

df_categorical <- df[, chr_columns_help]
print_unique_values(df_categorical)
       
```

Let's define ordered categorical variables:

```{r}
ordered_categorical = c("checking_status", "employment", "job", "savings_status")
head(df[, ordered_categorical])
```

We found out that the group unemp/unskilled non res has just 40 observations, 29 with good class and 11 with bad. The distribution of this class is very similar to the distribution of unskilled resident.

unemp/unskilled non res:
29/40 = 0.725 - share of good credits in this group

unskilled resident:
288/411 = 0.700

Because of that we decide to join these two groups.

```{r, message=FALSE}
df %>%
dplyr::group_by(job, class) %>%
dplyr::summarise(count = n())
```


Define values for each category

```{r}
checking_status_mapping <- c(
  "no checking" = 1,
  "<0" = 2, 
  "0<=X<200" = 3, 
  ">=200" = 4 
  )

employment_mapping <- c(
  "unemployed" = 1, 
  "<1" = 2, 
  "1<=X<4" = 3, 
  "4<=X<7"= 4, 
  ">=7" = 5)

job_mapping <- c(
    "unemp/unskilled non res" = 1,
    "unskilled resident" = 1,
    "skilled" = 2,
    "high qualif/self emp/mgmt" = 3
)

savings_status_mapping <- c(
    "no known savings" = 1,
    "<100" = 2,
    "100<=X<500" = 3,
    "500<=X<1000" = 4,
    ">=1000" = 5
)

# Define the mapping function
recode_variable <- function(variable, mapping) {
  variable <- factor(variable, levels = names(mapping))
  return(as.numeric(variable))
}

df$checking_status <- recode_variable(df$checking_status, checking_status_mapping)
df$employment <- recode_variable(df$employment, employment_mapping)
df$job <- recode_variable(df$job, job_mapping)
df$savings_status <- recode_variable(df$savings_status, savings_status_mapping)
```

Update the chr_columns_help - delete ordered categorical columns and print

```{r}
# Remove ordered_categorical variables names from list
chr_columns_help <- setdiff(chr_columns_help, ordered_categorical)
```

#### Boolean variables

Let’s define boolean variables:

```{r}
boolean_variables = c("class", "foreign_worker", "own_telephone")
head(df[, boolean_variables])
```

```{r}
df$class <- ifelse(grepl("good", df$class), 1, 0)
df$foreign_worker <- ifelse(grepl("yes", df$foreign_worker), 1, 0)
df$own_telephone <- ifelse(grepl("yes", df$own_telephone), 1, 0)
```

```{r}
# Remove boolean_variables variables names from list
chr_columns_help <- setdiff(chr_columns_help, boolean_variables)
```

#### One-hot encoding

For the remaining columns we apply one-hot encodig

```{r}
# Print remaining categorical variables and its unique values
df_categorical <- df[, chr_columns_help]
print_unique_values(df_categorical)
```
We use dummy_cols function from fastDummies library

```{r}
df <- dummy_cols(
  df, 
  select_columns = chr_columns_help, 
  remove_first_dummy = TRUE,           # remove the first dummy to avoid the issue of multicollinearity
  remove_selected_columns = TRUE)     # remove the original columns
```

### Summary of data cleaning

We performed data cleaning in a few steps:

* Delete quotations (clean chr variables)
* Handle categorical variables divided into:
  + Personal status column - divde column into 2 columns
  + Ordered categorical variables
  + Boolean variables
  + Categorical variable (one-hot encoding)
  
After this transformations we obtained dataframe with 51 variables.

As we can see below, after the data transformation there are no missing values, so the data transformation process was successful.

```{r}
## Getting the number of missing values in each column
num_missing <-  colSums(is.na(df))

# Excluding columns that contains 0 missing values
num_missing <-  num_missing[num_missing > 0]
num_missing
```

Below we can see the new dataframe and its columns along with their value types

```{r, echo=FALSE}
datatable(df, options = list(
  searching = FALSE,
  pageLength = 5,
  lengthMenu = c(5, 10, 15, 20),
  scrollX = TRUE
), width = "100%")  # Adjust the width as needed
```


```{r, max.height='150px'}
str(df)
```

### Splitting the dataset

Before performing Exploratory Data Analysis we should divide our dataset into train and test, we do it as below. Furthermore, we remove the ID column (first column) since it will not be used in the further analysis.

```{r}
set.seed(123456789)
training_obs <- createDataPartition(df$id, 
                                    p = 0.7, 
                                    list = FALSE) 
df.train <- df[training_obs,]
df.test  <- df[-training_obs,]

df <- df[, -1]
df.train <- df.train[, -1]
df.test <- df.test[, -1]
```

Let's check the dataframe

```{r, echo=FALSE}
datatable(df, options = list(
  searching = FALSE,
  pageLength = 5,
  lengthMenu = c(5, 10, 15, 20),
  scrollX = TRUE
), width = "100%")  # Adjust the width as needed
```

Before writing the dfs let's change the order of columns, so the target variable (class) would be in the first column.

```{r}
# Move "class" from 3rd to the first position
df <- df[, c(3, 1:2, 4:ncol(df))]
df.train <- df.train[, c(3, 1:2, 4:ncol(df.train))]
df.test <- df.test[, c(3, 1:2, 4:ncol(df.train))]
```

Last check of the dataset before saving

```{r, echo=FALSE}
datatable(df.train, options = list(
  searching = FALSE,
  pageLength = 5,
  lengthMenu = c(5, 10, 15, 20),
  scrollX = TRUE
), width = "100%")  # Adjust the width as needed
```

Write dataframes

```{r}
# remove unecessary variables
rm(list = setdiff(ls(), c("df", "df.test", "df.train", "print_unique_values")))

# write df.train and df.test
write.csv(df.test, "./data/output/df.test.csv", row.names = FALSE)
write.csv(df.train, "./data/output/df.train.csv", row.names = FALSE)
write.csv(df, "./data/output/df.csv", row.names = FALSE)
```

## Exploratory Data Analysis

The EDA will be performed on training dataframe.

Divide variables according to their type

```{r, max.height='150px'}
# define numeric variables
numeric_variables <- c("age",
                       "checking_status",
                       "credit_amount",
                       "duration",
                       "employment",
                       "existing_credits",
                       "installment_commitment",
                       "num_dependents",
                       "residence_since")

# define feat variables - some "mysterious" variables prepared by our lecturer
feat_variables <- c()
for (i in 1:10){
  if (i < 10){
    var <- paste("feat0", i, sep="")
  } else {
    var <- paste("feat", i, sep="")
  }
  feat_variables <- append(feat_variables, var)
}

# define ordered_categorical
ordered_categorical = c("checking_status", "employment", "job", "savings_status")

# define boolean variables
boolean_variables <- setdiff(colnames(df.train), c(numeric_variables, feat_variables, ordered_categorical))

# check if the division was made properly
print_unique_values(df.train[,boolean_variables])
```

### Histograms of numeric variables

```{r}
par(mfrow=c(3, 4))

for (var in numeric_variables) {
    hist(df.train[[var]], main=paste("Histogram of", var), xlab=var)
}
```

### Histograms of feat variables
```{r}
par(mfrow=c(3, 4))

for (var in feat_variables) {
    hist(df.train[[var]], main=paste("Histogram of", var), xlab=var)
}
```

### Correlation matrix

For numeric variables and target variable.

```{r, message=FALSE}
library(corrplot)
```

```{r}
num_var <- c(numeric_variables, feat_variables, "class")
numeric_df <- df.train[, num_var]
cor_matrix <- cor(numeric_df, use="complete.obs")
corrplot(cor_matrix, method="circle")
```

### Barplots of categorical variables

```{r}
# For ordered categorical variables
par(mfrow=c(2, 2))
for (var in ordered_categorical) {
  counts <- table(df.train[[var]])
  barplot(counts, main=paste("Bar Plot of", var), xlab=var, ylab="Count")
}
```

### Boolean variables

In case of boolean variables we create table with distribution of every variable.

```{r, max.height='150px'}
# Creating a summary table for boolean variables with percentages
boolean_summary <- sapply(df.train[, boolean_variables], function(x) {
  tab <- table(factor(x, levels = c(0, 1)))
  pct <- 100 * tab / sum(tab) # Convert counts to percentage
  return(pct)
})

boolean_summary <- t(boolean_summary) # Transposing to have variables as rows and percentage columns

# Formatting the table for better readability
boolean_summary <- round(boolean_summary, 1) # Round to two decimal places

# Printing the summary table
print(boolean_summary)
```

### The distribution of target variable

As we can see below, the train and test dataframes are split proportionally, however we can observe unequal distribution - there are more than twice observations with class "1" (good).

```{r}
# Target variable distribution in df:
round(table(df$class)/length(df$class), 3)
```

```{r}
# Target variabledistribution in df.train:
round(table(df.train$class)/length(df.train$class), 3)
```

```{r}
# Target variabledistribution in df.test:
round(table(df.test$class)/length(df.test$class), 3)
```

## Feature selection with RFE

### Intro to RFE

Recursive Feature Elimination (RFE) is a feature selection method used to identify and select a subset of relevant features for model building. The method works by recursively removing features, building a model with the remaining features, and calculating model accuracy. The least important features are removed in each iteration until the specified number of features is reached.

In this section of the project, we employ RFE to optimize the feature set for predicting the class variable in our dataset. The process is outlined as follows.

First of all we clear the environment, load train and test df and new libraries.

```{r, message=FALSE}
rm(list=ls())
library(mlbench)
library(caret)
library(recipes)
library(doParallel)
```

```{r}
df.train <- read.csv("./data/output/df.train.csv")
df.test <- read.csv("./data/output/df.test.csv")

# rfe requires factor as target variable, so we modify class (1st column) - only on df.train dataframe as of now
df.train[, 1] <- as.factor(df.train[,1])
```

Detect the number of CPU cores available on the current machine and assign it to 'cores'. Then, register the detected number of cores for parallel processing using the doParallel package. This enables parallel execution of code to improve performance, especially for operations that can be executed concurrently.

```{r}
cores=detectCores()
registerDoParallel(cores=cores)
```

### Define seeds for rfeControl

If we use doParallel package and want to make our code reproducible we should pass seeds argument to rfeControl function it requires a list where each element corresponds to a specific resampling iteration's seed and the last element is a seed for the overall process.

Assuming 10-fold CV, we need a list of 11 elements: 10 for each fold and 1 for the overall process.

```{r}
set.seed(123456789)
numFolds <- 10
seedsList <- lapply(1:numFolds, function(x) sample.int(1000, 49)) # Generate random seeds for each fold

set.seed(123456789)
seedsList[[length(seedsList) + 1]] <- sample.int(1000, 1) # Add one more seed for the overall process
```


```{r}
## Define rfeControl object
control <- rfeControl(functions=rfFuncs, method="cv", number=10, seeds = seedsList)
```

### Perform feature selection with rfe
```{r}
set.seed(123456789)
results <- rfe(df.train[, seq(2, length(df.train), 1)],             # predictor variables 
               df.train[, 1],                                       # target variable for prediction
               sizes=c(1:49),                                       # the subset sizes to evaluate
               rfeControl=control)

```

Basing on the RFE method 35 variables have been chosen as the best predictors. Below we can see them.

```{r, max.height='150px'}
predictors(results)
```
Below we can see the plot of accuracy results vs number of variables included. Interesting is the fact that having more predictors than 35 not only does not increase precision but also worsens it.

```{r}
plot(results, type=c("g", "o"))
```

Assign the final columns to list and write as .csv.

```{r}
rfe_columns <- predictors(results)
write.csv(rfe_columns, "./data/output/rfe_columns.csv")
```

## XGB - one by one

In this section we apply the approach from classes adjusted to our case. We build the xgb model by hypertuning parameters one by one in the following order:

* nrounds,
* max_depth and min_child_weight,
* colsample_bytree,
* subsample,
* change learning rate and number of trees,
* change learning rate and number of trees again.

Let's start with loading required packages, cleaning the environment and loading the data.

```{r, message=FALSE}
library(dplyr)
library(rpart)
library(rpart.plot)
library(xgboost)
library(plyr)
library(doParallel)
library(pROC)
library(ggplot2)
library(caret)

cores=detectCores()
registerDoParallel(cores=cores)
```

```{r}
rm(list=ls())

df <- read.csv("./data/output/df.csv")
df.train <- read.csv("./data/output/df.train.csv")
df.test <- read.csv("./data/output/df.test.csv")
```

Xgb requires target variable as factor - let's change it

```{r}
df$class <- factor(df$class, levels = c(0, 1), labels = c("bad", "good"))
df.train$class <- factor(df.train$class, levels = c(0, 1), labels = c("bad", "good"))
df.test$class <- factor(df.test$class, levels = c(0, 1), labels = c("bad", "good"))
```

Take columns chosen by RFE

```{r}
# Read columns chosen by rfe
rfe_columns <- read.csv("./data/output/rfe_columns.csv")

# Add "class" to the list of columns chosen for analysis
rfe_columns <- c("class", rfe_columns$x)

# Modify dfs to have columns chosen by rfe
df.train <- df.train[, rfe_columns]
df.test <- df.test[, rfe_columns]
```

Calculate initial parameters based on recommendations provided during the lectures.

```{r}
# colsample_bytree - # rule of thumb (number of predictors)
sqrt(35)/35.  # 0.1690309

# min_child_weight - 0.5 ... 0.1% of number of observations
0.005 * 2000  # 10
0.01 * 2000   # 20
```


### Hyperparameter tuning

#### Nrounds

```{r, warning=FALSE}
set.seed(12345678)
xgb_grid <- expand.grid(nrounds = seq(20, 80, 10),
                              max_depth = c(8),
                              eta = c(0.25),
                              gamma = 1,
                              colsample_bytree = c(0.17),
                              min_child_weight = c(10),
                              subsample = 0.8)

train_control <- trainControl(method = "cv", 
                              number = 5,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)

xgb_model.1 <- train(class ~ .,
                   data = df.train,
                   method = "xgbTree",
                   trControl = train_control,
                   tuneGrid  = xgb_grid)
```

```{r, max.height='150px'}
xgb_model.1
```
```{r}
xgb_model.1 %>% saveRDS("./data/output/xgb/xgb_model.1.rds")
```

The best for nrounds = 40
Let's take this parameter and tuner others.

#### Max_depth and min_child_weight

```{r, message = FALSE}
xgb_grid <- expand.grid(nrounds = 40,
                        max_depth = c(5, 15, 1),
                        eta = c(0.25),
                        gamma = 1,
                        colsample_bytree = c(0.17),
                        min_child_weight = seq(10, 200, 2),
                        subsample = 0.8)
set.seed(12345678)
xgb_model.2 <- train(class ~ .,
                     data = df.train,
                     method = "xgbTree",
                     trControl = train_control,
                     tuneGrid  = xgb_grid)
```

```{r, max.height='150px'}
xgb_model.2
```

```{r}
xgb_model.2 %>% saveRDS("./data/output/xgb/xgb_model.2.rds")
```

The best for max_depth = 5, min_child_weight = 12.
Let's take this parameter and tuner others.

#### Colsample_bytree

```{r}
xgb_grid <- expand.grid(nrounds = 40,
                        max_depth = 5,
                        eta = c(0.25),
                        gamma = 1,
                        colsample_bytree = seq(0.05, 0.7, 0.01),
                        min_child_weight = 12,
                        subsample = 0.8)

set.seed(12345678)
xgb_model.3 <- train(class ~ .,
                     data = df.train,
                     method = "xgbTree",
                     trControl = train_control,
                     tuneGrid  = xgb_grid)
```

```{r, max.height='150px'}
xgb_model.3
```

```{r}
xgb_model.3 %>% saveRDS("./data/output/xgb/xgb_model.3.rds")
```

The best for colsample_bytree = 0.65.
Let's take this parameter and tuner others.

#### Subsample

```{r, message=FALSE}
xgb_grid <- expand.grid(nrounds = 40,
                        max_depth = 5,
                        eta = c(0.25),
                        gamma = 1,
                        colsample_bytree = 0.65,
                        min_child_weight = 12,
                        subsample = seq(0.6, 0.9, 0.05))

set.seed(12345678)
xgb_model.4 <- train(class ~ .,
                     data = df.train,
                     method = "xgbTree",
                     trControl = train_control,
                     tuneGrid  = xgb_grid)
```

```{r, max.height='150px'}
xgb_model.4
```


```{r}
xgb_model.4 %>% saveRDS("./data/output/xgb/xgb_model.4.rds")
```

The best for subsample = 0.85.
Let's take this parameter and tuner others.

#### Learning rate and number of trees

Let's double number of trees and reduce by half learning rate

```{r}
# Change learning rate and number of trees
xgb_grid <- expand.grid(nrounds = 80,
                        max_depth = 5,
                        eta = c(0.12),
                        gamma = 1,
                        colsample_bytree = 0.65,
                        min_child_weight = 12,
                        subsample = 0.85)

set.seed(12345678)
xgb_model.5 <- train(class ~ .,
                     data = df.train,
                     method = "xgbTree",
                     trControl = train_control,
                     tuneGrid  = xgb_grid)
```

```{r}
xgb_model.5
```

```{r}
xgb_model.5 %>% saveRDS("./data/output/xgb/xgb_model.5.rds")
```

#### Learning rate and number of trees

Let's double number of trees and reduce by half learning rate again

```{r}
# Change learning rate and number of trees
xgb_grid <- expand.grid(nrounds = 160,
                        max_depth = 5,
                        eta = c(0.06),
                        gamma = 1,
                        colsample_bytree = 0.65,
                        min_child_weight = 12,
                        subsample = 0.85)

set.seed(12345678)
xgb_model.6 <- train(class ~ .,
                     data = df.train,
                     method = "xgbTree",
                     trControl = train_control,
                     tuneGrid  = xgb_grid)
```

```{r}
xgb_model.6
```

Compare models on test dataframe

```{r}
# Load function to take accuracy and gini
source("./scripts/getAccuracyAndGini.R")

models <- c("1":"6")

# Apply the function to every model and build table
sapply(paste0("xgb_model.", models),
       function(x) getAccuracyAndGini(model = get(x),
                                      data = df.test,
                                      target_variable = "class",
                                      predicted_class = "good")
)
```

xgb_model.5 has the highest values, so we choose this one as final for xgb.

### Calculate ROC

```{r, message=FALSE}
ROC.train <- pROC::roc(df.train$class, 
                       predict(xgb_model.6,
                               df.train, type = "prob")[, "good"])

ROC.test  <- pROC::roc(df.test$class, 
                       predict(xgb_model.6,
                               df.test, type = "prob")[, "good"])

cat("AUC for train = ", pROC::auc(ROC.train), 
    ", Gini for train = ", 2 * pROC::auc(ROC.train) - 1, "\n", sep = "")

cat("AUC for test = ", pROC::auc(ROC.test), 
    ", Gini for test = ", 2 * pROC::auc(ROC.test) - 1, "\n", sep = "")

```


### Chart of xgb ROC

```{r}
list(
  ROC.train   = ROC.train,
  ROC.test    = ROC.test
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(subtitle = paste0("Gini TRAIN: ",
                         "xgb = ", 
                         round(100 * (2 * auc(ROC.train) - 1), 1), "%, ",
                         "Gini TEST: ",
                         "xgb = ", 
                         round(100 * (2 * auc(ROC.test) - 1), 1), "%, "
                         )) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired")
```


## GBM

In this section we will perform GBM model.
Let's start with clearing the environment and preparing df.

```{r, message=FALSE}
library(gbm)
library(dplyr)
library(caret)
```


```{r}
rm(list=ls())

df <- read.csv("./data/output/df.csv")
df.train <- read.csv("./data/output/df.train.csv")
df.test <- read.csv("./data/output/df.test.csv")
```

```{r}
rfe_columns <- read.csv("./data/output/rfe_columns.csv")
rfe_columns <- c("class", rfe_columns$x)

df.train <- df.train[, rfe_columns]
df.test <- df.test[, rfe_columns]
```

```{r}
# Make factor
df$class <- factor(df$class, levels = c(0, 1), labels = c("bad", "good"))
df.train$class <- factor(df.train$class, levels = c(0, 1), labels = c("bad", "good"))
df.test$class <- factor(df.test$class, levels = c(0, 1), labels = c("bad", "good"))
```

Function to define model formula

```{r}
# define model formula - all variables apart from class
create_model_formula <- function(df, explained_variable){
  
  model_formula <- paste(explained_variable, "~")
  
  for (i in 2:length(colnames(df))){
    if(i == 2){
      model_formula <- paste(model_formula, colnames(df)[i])
    } else {
      model_formula <- paste(model_formula, " + ", colnames(df)[i])
    }
  }
  
  return(as.formula(model_formula))
}
```

Define 2 model formulas - one for class (factor) and one for class_1 (number).

```{r}
model_formula01 <- create_model_formula(df.train, "class_1")
model_formula <- create_model_formula(df.train, "class")
```

GBM requires target variable to be defined as 0 and 1, not factor so we do not change it. However we will also need variable "class" with factor.
Define new variable - class_1 with 0 and 1.

```{r}
df.train$class_1 <- (df.train$class == "good") * 1
df.test$class_1 <- (df.test$class == "good") * 1
```


### Simple GBM

In this section we will build simple GBM modelm without any hyperparameter tuning.

```{r}
set.seed(123456789)
gbm.1 <- 
  gbm(model_formula01,
      data = df.train,
      distribution = "bernoulli",
      # total number of trees
      n.trees = 500,
      # number of variable interactions - actually depth of the trees
      interaction.depth = 4,
      # shrinkage parameter - speed (pace) of learning
      shrinkage = 0.01,
      verbose = FALSE)

gbm.1 %>% saveRDS("./data/output/gbm/gbm.1.rds")

# predict train
df.pred.train.gbm <- predict(gbm.1,
                                  df.train, 
                                  type = "response",
                                  n.trees = 500)
# predict test
df.pred.test.gbm <- predict(gbm.1,
                             df.test, 
                             type = "response",
                             n.trees = 500)
```

```{r, message=FALSE, warning=FALSE}
# Load function to get accuracy and gini values
source("./scripts/getAccuracyAndGini2.R")

# Training set
getAccuracyAndGini2(data = data.frame(class = df.train$class,
                                      pred = df.pred.train.gbm),
                    predicted_probs = "pred",
                    target_variable = "class",
                    target_levels = c("good", "bad"),
                    predicted_class = "good")
```

```{r, warning=FALSE}
# Test set
getAccuracyAndGini2(data = data.frame(class = df.test$class,
                                      pred = df.pred.test.gbm),
                    predicted_probs = "pred",
                    target_variable = "class",
                    target_levels = c("good", "bad"),
                    predicted_class = "good")
```

```{r, message=FALSE}
ROC.train.gbm <- pROC::roc(df.train$class, 
                           df.pred.train.gbm)

ROC.test.gbm  <- pROC::roc(df.test$class, 
                           df.pred.test.gbm)

cat("AUC for train = ", pROC::auc(ROC.train.gbm), 
    ", Gini for train = ", 2 * pROC::auc(ROC.train.gbm) - 1, "\n", sep = "")

cat("AUC for test = ", pROC::auc(ROC.test.gbm), 
    ", Gini for test = ", 2 * pROC::auc(ROC.test.gbm) - 1, "\n", sep = "")
```

Plot ROC

```{r}
list(
  ROC.train.gbm = ROC.train.gbm,
  ROC.test.gbm  = ROC.test.gbm
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(subtitle = paste0("Gini TRAIN: ",
                         "gbm = ", 
                         round(100 * (2 * auc(ROC.train.gbm) - 1), 1), "%, ",
                         "Gini TEST: ",
                         "gbm = ", 
                         round(100 * (2 * auc(ROC.test.gbm) - 1), 1), "%, "
  )) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired")
```


### Hyperparameter tuning

```{r}
parameters_gbm <- expand.grid(interaction.depth = c(1, 2, 4),
                              n.trees = c(100, 300, 500),
                              shrinkage = c(0.01, 0.1), 
                              n.minobsinnode = c(150, 250, 400))

ctrl_cv3 <- trainControl(method = "cv", 
                         number = 10,
                         classProbs = TRUE,
                         summaryFunction = twoClassSummary)
```

```{r, max.height='150px', warning=FALSE}
set.seed(123456789)
gbm.2  <- train(model_formula,
                       data = df.train,
                       distribution = "bernoulli",
                       method = "gbm",
                       tuneGrid = parameters_gbm,
                       trControl = ctrl_cv3,
                       verbose = FALSE)

gbm.2
```


```{r}
res <- gbm.2$results

saveRDS(object = gbm.2,
        file   = "./data/output/gbm/gbm.2.rds")
```

```{r}
# Predict train
df.pred.train.gbm2 <- predict(gbm.2,
                             df.train, 
                             type = "prob",
                             n.trees = 500)
# Predict test
df.pred.test.gbm2 <- predict(gbm.2,
                            df.test, 
                            type = "prob",
                            n.trees = 500)
```

```{r, warning=FALSE}
# Training set
accuracy_gini_train <- getAccuracyAndGini2(data = data.frame(class = df.train$class,
                                      pred = df.pred.train.gbm2[, "good"]),
                    predicted_probs = "pred",
                    target_variable = "class",
                    target_levels = c("good", "bad"),
                    predicted_class = "good")
```

```{r, warning=FALSE}
# Test set
accuracy_gini_test <- getAccuracyAndGini2(data = data.frame(class = df.test$class,
                                      pred = df.pred.test.gbm2[, "good"]),
                    predicted_probs = "pred",
                    target_variable = "class",
                    target_levels = c("good", "bad"),
                    predicted_class = "good")
```

```{r}
write.csv(accuracy_gini_test, "./data/accuracy_gini_test.csv")
write.csv(accuracy_gini_train, "./data/accuracy_gini_train.csv")
```

```{r, message=FALSE}
ROC.train.gbm2 <- pROC::roc(df.train$class, 
                           df.pred.train.gbm2[, "good"])

ROC.test.gbm2  <- pROC::roc(df.test$class, 
                           df.pred.test.gbm2[, "good"])
```

```{r}
cat("AUC for train = ", pROC::auc(ROC.train.gbm2), 
    ", Gini for train = ", 2 * pROC::auc(ROC.train.gbm2) - 1, "\n", sep = "")

cat("AUC for test = ", pROC::auc(ROC.test.gbm2), 
    ", Gini for test = ", 2 * pROC::auc(ROC.test.gbm2) - 1, "\n", sep = "")
```

```{r}
list(
  ROC.train.gbm2 = ROC.train.gbm2,
  ROC.test.gbm2  = ROC.test.gbm2
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(subtitle = paste0("Gini TRAIN: ",
                         "gbm = ", 
                         round(100 * (2 * auc(ROC.train.gbm2) - 1), 1), "%, ",
                         "Gini TEST: ",
                         "gbm = ", 
                         round(100 * (2 * auc(ROC.test.gbm2) - 1), 1), "%, "
  )) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired")
```


## Random Forest - one by one

Let's start with cleaning the environment and loading the data.

```{r}
rm(list=ls())

df <- read.csv("./data/output/df.csv")
df.train <- read.csv("./data/output/df.train.csv")
df.test <- read.csv("./data/output/df.test.csv")

df.train$class <- factor(df.train$class, levels = c(0, 1), labels = c("bad", "good"))
df.test$class <- factor(df.test$class, levels = c(0, 1), labels = c("bad", "good"))
```

Prepare df as before.

```{r}
rfe_columns <- read.csv("./data/output/rfe_columns.csv")
rfe_columns <- c("class", rfe_columns$x)

df.train <- df.train[, rfe_columns]
df.test <- df.test[, rfe_columns]
```

### rf

Estimate first - basic model with all variables and default parameters

```{r, message=FALSE}
library(randomForest)
library(caret)
library(pROC)
library(dplyr)
```

```{r}
set.seed(123456789)
rf <- randomForest(class ~ ., 
                   data = df.train)

saveRDS(rf, file = "./data/output/rf/rf.RDS")
```

```{r}
print(rf)
```


Estimated out-of-bag-error: 13.57%

```{r}
plot(rf)
```

The plot above helps determine the appropriate number of trees.

The dark solid line presents the total OOB error, the red line presents the prediction error for "good" response, while the green line for "bad" response.

As the number of trees is higher, the OOB error is smaller, and converges to approx. 120 trees.

### rf2

Hence, let us try to limit number of trees and estimate the model on bootstrap samples from the full data set.

We also increase the number of predictors used from 5 to 10.

```{r}
set.seed(123456789)
rf2 <- 
  randomForest(class ~ .,
               data = df.train,
               ntree = 120,
               sampsize = nrow(df.train),
               mtry = 10,
               # minimum number of obs in the terminal nodes
               nodesize = 100,
               # we also generate predictors importance measures,
               importance = TRUE)

saveRDS(rf2, file = "./data/output/rf/rf2.RDS")
print(rf2)
```

### rf3 - cross validation

Let us try to optimize the mtry parameter using the cross-validation process.

We consider 35 predictors in the model formula. Hence, let us try mtry between 2 and 31.

```{r, max.height='150px'}
parameters_rf <- expand.grid(mtry = 2:31)
ctrl_oob <- trainControl(method = "oob", classProbs = TRUE)

set.seed(123456789)

rf3 <-
  train(class ~ .,
        data = df.train,
        method = "rf",
        ntree = 120,
        nodesize = 100,
        tuneGrid = parameters_rf,
        trControl = ctrl_oob,
        importance = TRUE)

rf3
saveRDS(rf3, file = "./data/output/rf/rf3.RDS") 
```

```{r}
plot(rf3$results$mtry,
     rf3$results$Accuracy, type = "b")
```

```{r}
plot(rf3$results$mtry,
     rf3$results$Kappa, type = "b")
```

```{r}
modelLookup("ranger")
```

### rf3a - grid search

Let's apply grid search and try to optimize mtry and min.node.size parameters

```{r}
parameters_ranger <- 
  expand.grid(mtry = 4:15,
              # split rule
              splitrule = "gini",
              # minimum size of the terminal node
              min.node.size = c(50, 100, 150))

ctrl_cv5 <- trainControl(method = "cv", 
                         number =    5,
                         classProbs = T)

set.seed(123456789)
rf3a <- 
  train(class ~ ., 
        data = df.train, 
        method = "ranger", 
        num.trees = 120, # default = 500
        # numbers of processor cores to use in computations
        num.threads = 3,
        # impurity measure
        importance = "impurity",
        # parameters
        tuneGrid = parameters_ranger, 
        trControl = ctrl_cv5)

saveRDS(rf3a, file = "./data/output/rf/rf3a.RDS") 
```

```{r, max.height='150px'}
rf3a
```

```{r}
plot(rf3a)
```


In this case the omptimal value for mtry is 6.

Let's compare all of the models.

```{r, message=FALSE}
pred.train.rf <- predict(rf, 
                         df.train, 
                         type = "prob")[, "good"]
ROC.train.rf  <- roc(as.numeric(df.train$class == "good"), 
                     pred.train.rf)


pred.test.rf  <- predict(rf, 
                         df.test, 
                         type = "prob")[, "good"]
ROC.test.rf   <- roc(as.numeric(df.test$class == "good"), 
                     pred.test.rf)


pred.train.rf2 <- predict(rf2, 
                          df.train, 
                          type = "prob")[, "good"]
ROC.train.rf2  <- roc(as.numeric(df.train$class == "good"), 
                      pred.train.rf2)


pred.test.rf2  <- predict(rf2, 
                          df.test, 
                          type = "prob")[, "good"]
ROC.test.rf2   <- roc(as.numeric(df.test$class == "good"), 
                      pred.test.rf2)


pred.train.rf3 <- predict(rf3, 
                          df.train, 
                          type = "prob")[, "good"]
ROC.train.rf3  <- roc(as.numeric(df.train$class == "good"), 
                      pred.train.rf3)


pred.test.rf3  <- predict(rf3, 
                          df.test, 
                          type = "prob")[, "good"]
ROC.test.rf3   <- roc(as.numeric(df.test$class == "good"), 
                      pred.test.rf3)


pred.train.rf3a <- predict(rf3a, 
                           df.train, 
                           type = "prob")[, "good"]
ROC.train.rf3a  <- roc(as.numeric(df.train$class == "good"), 
                       pred.train.rf3a)


pred.test.rf3a  <- predict(rf3a, 
                           df.test, 
                           type = "prob")[, "good"]
ROC.test.rf3a   <- roc(as.numeric(df.test$class == "good"), 
                       pred.test.rf3a)
```

```{r}
list(
  ROC.train.rf   = ROC.train.rf,
  ROC.test.rf    = ROC.test.rf,
  ROC.train.rf2  = ROC.train.rf2,
  ROC.test.rf2   = ROC.test.rf2,
  ROC.train.rf3  = ROC.train.rf3,
  ROC.test.rf3   = ROC.test.rf3,
  ROC.train.rf3a = ROC.train.rf3a,
  ROC.test.rf3a  = ROC.test.rf3a
) %>%
  pROC::ggroc(alpha = 0.5, linetype = 1, size = 1) + 
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color = "grey", 
               linetype = "dashed") +
  labs(title = paste0("Gini TEST: ",
                      "rf = ", 
                      round(100 * (2 * auc(ROC.test.rf) - 1), 1), "%, ",
                      "rf2 = ", 
                      round(100 * (2 * auc(ROC.test.rf2) - 1), 1), "%, ",
                      "rf3 = ", 
                      round(100 * (2 * auc(ROC.test.rf3) - 1), 1), "%, ",
                      "rf3a = ", 
                      round(100 * (2 * auc(ROC.test.rf3a) - 1), 1), "% "),
       subtitle =  paste0("Gini TRAIN: ",
                          "rf = ", 
                          round(100 * (2 * auc(ROC.train.rf) - 1), 1), "%, ",
                          "rf2 = ", 
                          round(100 * (2 * auc(ROC.train.rf2) - 1), 1), "%, ",
                          "rf3 = ", 
                          round(100 * (2 * auc(ROC.train.rf3) - 1), 1), "%, ",
                          "rf3a = ", 
                          round(100 * (2 * auc(ROC.train.rf3a) - 1), 1), "% ")) +
  theme_bw() + coord_fixed() +
  scale_color_brewer(palette = "Paired")
```

Finally the random forest model with grid search (rf3a) with mtry = 6, splitrule = gini and min.node.size = 50 turned out to be the best.


## Hyperparameter tuning using tidymodels

In this section we tuned hyperparameters using tidymodels and tried to do it in more automated way.

In the first step we tune hyperparameters and take the best model parameters.

We built a function tune_two_stages (defined in a separate script) which takes the 2 grids of parameters and tunes models in two steps. 
Firstly it tunes one parameter, takes the best one and than it goes to the second grid tuning all parameters at the same time.
Results are sorted by -mean value and parameters of three best models are taken and written as csv.

In the second step we take the results and check the performance of the models.

We built the functions summary_xgb, summary_random_forest, summary_decision_tree which takes the 3 best set of parameters and their names and check the performance of the model.
We estimate the models using specific parameters on df.train, make predictions on df.train, df.test and check the performance using GINI and saving roc plots.


Let's start with cleaning the environment and loading the data.

```{r, message=FALSE}
library(dplyr)
library(rpart)
library(rpart.plot)
library(xgboost)
library(plyr)
library(doParallel)
library(pROC)
library(ggplot2)
library(gbm)
library(caret)
library(workflows)
library(parsnip)
library(recipes)
library(dials)
library(rsample)
library(tune)
library(yardstick)
library(DT)

cores=detectCores()
registerDoParallel(cores=cores)
```

```{r}
rm(list=ls())

cores=delightgbmcores=detectCores()
registerDoParallel(cores=cores)

source("./scripts/tune_functions.R")

df <- read.csv("./data/output/df.csv")
df.train <- read.csv("./data/output/df.train.csv")
df.test <- read.csv("./data/output/df.test.csv")

# Make factor
df$class <- factor(df$class, levels = c(0, 1), labels = c("bad", "good"))
df.train$class <- factor(df.train$class, levels = c(0, 1), labels = c("bad", "good"))
df.test$class <- factor(df.test$class, levels = c(0, 1), labels = c("bad", "good"))

# Load rfe columns
rfe_columns <- read.csv("./data/output/rfe_columns.csv")
rfe_columns <- c("class", rfe_columns$x)

# Adjust dfs to have rfe columns
df.train <- df.train[, rfe_columns]
df.test <- df.test[, rfe_columns]

# Load functions from tune_functions script
source("./scripts/tune_functions.R")
```

### Define models

Define models and their hyperpatemeters to check.

```{r}
# define parameters of models
models <- list(
  xgboost = list(
    model_name = 'xgboost',
    
    model_1 = boost_tree(mode = "classification", engine = "xgboost", learn_rate = tune()),
    
    model_2_args = list(tree_depth = tune(), 
                        loss_reduction = tune(), 
                        trees = tune(),
                        stop_iter = tune(),
                        min_n = tune()),
    
    rec_spec = recipe(class ~ ., df.train),
    
    grid_1 = set_seed_grid(grid_latin_hypercube(learn_rate(), size = 10), 123),
    
    grid_2 = set_seed_grid(as_tibble(expand.grid(
      tree_depth = c(6, 9, 12),
      loss_reduction = c(0.5, 1, 2),
      trees = c(100, 300, 500),
      stop_iter = seq(10, 50, by=10),
      min_n = seq(25, 100, by=25)
    )), 123)
  ),
  random_forest = list(
    model_name = 'random_forest',
    model_1 = rand_forest(mode = "classification", engine = "ranger", mtry = 5, trees = tune()),
    model_2_args = list(min_n = tune()),
    
    rec_spec = recipe(class ~ ., df.train),
    
    grid_1 = set_seed_grid(grid_latin_hypercube(trees(), size = 50), 123),
    
    grid_2 = set_seed_grid(as_tibble(expand.grid(
      min_n = c(75, 100, 150, 300)
    )), 123)
  ),
  decision_tree = list(
    model_name = 'decision_tree',
    model_1 = decision_tree(mode = "classification", engine = "rpart", cost_complexity = tune()),
    model_2_args = list(tree_depth = tune(), min_n = tune()),
    
    rec_spec = recipe(class ~ ., df.train),
    
    grid_1 = set_seed_grid(grid_latin_hypercube(cost_complexity(), size = 10), 123),
    grid_2 = set_seed_grid(as_tibble(expand.grid(
      tree_depth = c(6, 9, 12),
      min_n = seq(25, 100, by=25)
    )), 123)
  )
)
```

### Tune xgb

```{r, message=FALSE, results='hide'}
# xgb tune
result_list_xgb <- list(tune_two_stages(models, "xgboost"))
result_list_xgb <- result_list_xgb[[1]]

# xgb summarise
tuned_params_xgb <- c("trees", "min_n", "tree_depth", "loss_reduction", "stop_iter", "learn_rate")
results <- result_list_xgb$result

final_results_xgb <- summary_xgb(results, tuned_params_xgb)
write.csv(final_results_xgb, "./data/output/xgb/final_results_xgb.csv")

```


### Tune rf

```{r, message=FALSE, results='hide'}
result_list_random_forest <- list(tune_two_stages(models, "random_forest"))
result_list_random_forest <- result_list_random_forest[[1]]

# rf summarise
tuned_params_random_forest <- c("trees", "min_n")
results <- result_list_random_forest$result

final_results_random_forest <- summary_random_forest(results, tuned_params_random_forest)
write.csv(final_results_random_forest, "./data/output/rf/final_results_random_forest.csv")
```


### Tune decision tree

```{r, message=FALSE, results='hide'}
result_decision_tree <- list(tune_two_stages(models, "decision_tree"))
result_decision_tree <- result_decision_tree[[1]]

# decision_tree summarise
tuned_params_decision_tree <- c("cost_complexity", "tree_depth", "min_n")
results <- result_decision_tree$result

final_results_decision_tree <- summary_decision_tree(results, tuned_params_decision_tree)
write.csv(final_results_decision_tree, "./data/output/dt/final_results_decision_tree.csv")
```

### Compare results

```{r}
columns_to_compare <- c("iteration", "gini_train", "gini_test", "models_spec", "model_name")

final_results_xgb_c <- final_results_xgb[, columns_to_compare]
final_results_random_forest_c <- final_results_random_forest[, columns_to_compare]
final_results_decision_tree_c <- final_results_decision_tree[, columns_to_compare]
```

```{r}
final_results_all <- rbind(final_results_xgb_c, final_results_random_forest_c, final_results_decision_tree_c) %>% arrange(-gini_test)
```

```{r, echo=FALSE}
datatable(final_results_all, options = list(
  searching = FALSE,
  pageLength = 10,
  lengthMenu = c(5, 10, 15, 20),
  scrollX = TRUE
), width = "100%")  # Adjust the width as needed
```

Plot ROC for the best model - rf1.

```{r}
knitr::include_graphics("./plots/rf_1.png")
```

## Summary

To sum up, we built several ML models using a few techniques. 
Finally, it turned out that the xgb model tuned by "one by one" method is the best with gini 92.1% on the df.train and 77% on df.test.

Random forest model tuned in more automated way, presented in the table above also turned out to have relatively high gini values (91.2% train, 77.7% df.test).

All results are presented in the attached pdf presentation.

Even though we used a few techniques which might increase the models accuracy and efficiency like handling missing values, transforming columns according to their characteristics, feature selection using Recursive Feature Elimination, cross validation, grid search and so on we believe that the accuracy might be even more increased for instance by using over or undersampling or some other methods, which we did not apply.