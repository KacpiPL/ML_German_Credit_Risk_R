---
title: "ML - German_Credit_Risk"
author: "Bryzik Michał, Gruca Kacper"
date: "2024-01-09"
output: 
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE}
options(width = 60)
local({
  hook_output <- knitr::knit_hooks$get('output')
  knitr::knit_hooks$set(output = function(x, options) {
    if (!is.null(options$max.height)) options$attr.output <- c(
      options$attr.output,
      sprintf('style="max-height: %s;"', options$max.height)
    )
    hook_output(x, options)
  })
})
```

## Introduction

Classification problem

The main goal of this paper is to apply ML methods, especially Decision Trees to the classify clients.


```{r, message=FALSE}
library(tidyverse)
library(DT)
library(dplyr)
library(fastDummies)
library(caret)
library(ggplot2)
library(plotfunctions)
```

## Data cleaning

Read csv
```{r}
df <- read.csv("data/input/c2.csv")
```

Below we can observe raw dataframe and its format of the columns:

```{r, echo=FALSE}
datatable(df, options = list(
  searching = FALSE,
  pageLength = 5,
  lengthMenu = c(5, 10, 15, 20),
  scrollX = TRUE
), width = "100%")  # Adjust the width as needed
```

```{r, max.height='150px'}
str(df)
```

### Delete quotations

As we can observe all chr columns have additiona quotes '', let's delete them.

```{r}
df <- df %>%
  mutate_if(is.character, function(x) gsub("'", "", x))
```

### Missing values

Dataframe does not have any missing values, as we can see below

```{r}
## Getting the number of missing values in each column
num_missing <-  colSums(is.na(df))

# Excluding columns that contains 0 missing values
num_missing <-  num_missing[num_missing > 0]
num_missing

```

### Handle Categorical variables

Select all columns with chr data type - we treat them as categorical

```{r}
chr_columns <- names(df[, sapply(df, is.character)])
chr_columns
```

Create df with categorical variables, print unique values of each column.

```{r}
# Function to print unique values for each column in dataframe
print_unique_values <- function(df){
  
  df_unique <- lapply(df, unique)
  # Print unique values for each column
  for (col_name in names(df_unique)) {
    cat(col_name, ": ", paste(df_unique[[col_name]], collapse = ", "), "\n")
  }
}

```

```{r}
df_categorical <- df[chr_columns]

print_unique_values(df_categorical)
       
```

#### Personal status column

Let's focus on the column "personal_status", here we have 2 information, sex and marital status

We should separate them into 2 columns:

* sex (0 - female, 1 - male)
* marital_status:
  + single
  + div/dep/mar
  + mar/wid
  + div/sep

Check the size of each group:

```{r}
personal_status_counts <- table(df$personal_status)

print(personal_status_counts)
```

Create 2 new columns:

```{r}
# Extracting sex information
df <- df %>%
  mutate(sex = ifelse(grepl("female", personal_status), 0, 1))

# Extracting the rest of the information
df <- df %>%
  mutate(marital_status = sub("^[^ ]+ ", "", sub(" $", "", personal_status)))

# Drop the original "personal_status" column
df <- df %>%
  select(-personal_status)
```

```{r}
sex_counts <- table(df$sex)
marital_status_counts <- table(df$marital_status)

print(sex_counts)
print(marital_status_counts)
```

Create a list of reamining chr columns which needs to be handled yet

```{r}
# Copy chr columns and remove personal_status - now we have list with remaining chr columns
chr_columns_help <- chr_columns

# Remove "personal_status" from list
chr_columns_help <- setdiff(chr_columns_help, "personal_status")

# Adding marital_status to object_columns_help
chr_columns_help <- union(chr_columns_help, "marital_status")

print(chr_columns_help)
```

#### Ordered categorical variables

In the dataset we can observe a few ordered categorical variables. Let's define and transform them.

```{r}
# Refresh the df_categorical and show unique values - columns to be handled yet

df_categorical <- df[, chr_columns_help]
print_unique_values(df_categorical)
       
```

Let's define ordered categorical variables:

```{r}
ordered_categorical = c("checking_status", "employment", "job", "savings_status")
head(df[, ordered_categorical])
```

We found out that the group unemp/unskilled non res has just 40 observations, 29 with good class and 11 with bad. The distribution of this class is very similar to the distribution of unskilled resident.

unemp/unskilled non res:
29/40 = 0.725 - share of good credits in this group

unskilled resident:
288/411 = 0.700

Because of that we decide to join these two groups.

```{r, message=FALSE}
df %>%
dplyr::group_by(job, class) %>%
dplyr::summarise(count = n())
```


Define values for each category

```{r}
checking_status_mapping <- c(
  "no checking" = 1,
  "<0" = 2, 
  "0<=X<200" = 3, 
  ">=200" = 4 
  )

employment_mapping <- c(
  "unemployed" = 1, 
  "<1" = 2, 
  "1<=X<4" = 3, 
  "4<=X<7"= 4, 
  ">=7" = 5)

job_mapping <- c(
    "unemp/unskilled non res" = 1,
    "unskilled resident" = 1,
    "skilled" = 2,
    "high qualif/self emp/mgmt" = 3
)

savings_status_mapping <- c(
    "no known savings" = 1,
    "<100" = 2,
    "100<=X<500" = 3,
    "500<=X<1000" = 4,
    ">=1000" = 5
)

# Define the mapping function
recode_variable <- function(variable, mapping) {
  variable <- factor(variable, levels = names(mapping))
  return(as.numeric(variable))
}

df$checking_status <- recode_variable(df$checking_status, checking_status_mapping)
df$employment <- recode_variable(df$employment, employment_mapping)
df$job <- recode_variable(df$job, job_mapping)
df$savings_status <- recode_variable(df$savings_status, savings_status_mapping)
```

Update the chr_columns_help - delete ordered categorical columns and print

```{r}
# Remove ordered_categorical variables names from list
chr_columns_help <- setdiff(chr_columns_help, ordered_categorical)
```

#### Boolean variables

Let’s define boolean variables:

```{r}
boolean_variables = c("class", "foreign_worker", "own_telephone")
head(df[, boolean_variables])
```

```{r}
df$class <- ifelse(grepl("good", df$class), 1, 0)
df$foreign_worker <- ifelse(grepl("yes", df$foreign_worker), 1, 0)
df$own_telephone <- ifelse(grepl("yes", df$own_telephone), 1, 0)
```

```{r}
# Remove boolean_variables variables names from list
chr_columns_help <- setdiff(chr_columns_help, boolean_variables)
```

#### One-hot encoding

For the remaining columns we apply one-hot encodig

```{r}
# Print remaining categorical variables and its unique values
df_categorical <- df[, chr_columns_help]
print_unique_values(df_categorical)
```
We use dummy_cols function from fastDummies library

```{r}
df <- dummy_cols(
  df, 
  select_columns = chr_columns_help, 
  remove_first_dummy = TRUE,           # remove the first dummy to avoid the issue of multicollinearity
  remove_selected_columns = TRUE)     # remove the original columns
```

### Summary of data cleaning

We performed data cleaning in a few steps:

* Delete quotations (clean chr variables)
* Handle categorical variables divided into:
  + Personal status column - divde column into 2 columns
  + Ordered categorical variables
  + Boolean variables
  + Categorical variable (one-hot encoding)
  
After this transformations we obtained dataframe with 51 variables.

As we can see below, after the data transformation there are no missing values, so the data transformation process was successful.

```{r}
## Getting the number of missing values in each column
num_missing <-  colSums(is.na(df))

# Excluding columns that contains 0 missing values
num_missing <-  num_missing[num_missing > 0]
num_missing
```

Below we can see the new dataframe and its columns along with their value types

```{r, echo=FALSE}
datatable(df, options = list(
  searching = FALSE,
  pageLength = 5,
  lengthMenu = c(5, 10, 15, 20),
  scrollX = TRUE
), width = "100%")  # Adjust the width as needed
```


```{r, max.height='150px'}
str(df)
```

### Splitting the dataset

Before performing Exploratory Data Analysis we should divide our dataset into train and test, we do it as below. Furthermore, we remove the ID column (first column) since it will not be used in the further analysis.

```{r}
set.seed(123456789)
training_obs <- createDataPartition(df$id, 
                                    p = 0.7, 
                                    list = FALSE) 
df.train <- df[training_obs,]
df.test  <- df[-training_obs,]

df <- df[, -1]
df.train <- df.train[, -1]
df.test <- df.test[, -1]
```

Let's check the dataframe

```{r, echo=FALSE}
datatable(df, options = list(
  searching = FALSE,
  pageLength = 5,
  lengthMenu = c(5, 10, 15, 20),
  scrollX = TRUE
), width = "100%")  # Adjust the width as needed
```

Before writing the dfs let's change the order of columns, so the target variable (class) would be in the first column.

```{r}
# Move "class" from 3rd to the first position
df <- df[, c(3, 1:2, 4:ncol(df))]
df.train <- df.train[, c(3, 1:2, 4:ncol(df.train))]
df.test <- df.test[, c(3, 1:2, 4:ncol(df.train))]
```

Last check of the dataset before saving

```{r, echo=FALSE}
datatable(df.train, options = list(
  searching = FALSE,
  pageLength = 5,
  lengthMenu = c(5, 10, 15, 20),
  scrollX = TRUE
), width = "100%")  # Adjust the width as needed
```

Write dataframes

```{r}
# remove unecessary variables
rm(list = setdiff(ls(), c("df", "df.test", "df.train", "print_unique_values")))

# write df.train and df.test
write.csv(df.test, "./data/output/df.test.csv", row.names = FALSE)
write.csv(df.train, "./data/output/df.train.csv", row.names = FALSE)
write.csv(df, "./data/output/df.csv", row.names = FALSE)
```

## Exploratory Data Analysis

The EDA will be performed on training dataframe.

Divide variables according to their type

```{r, max.height='150px'}
# define numeric variables
numeric_variables <- c("age",
                       "checking_status",
                       "credit_amount",
                       "duration",
                       "employment",
                       "existing_credits",
                       "installment_commitment",
                       "num_dependents",
                       "residence_since")

# define feat variables - some "mysterious" variables prepared by our lecturer
feat_variables <- c()
for (i in 1:10){
  if (i < 10){
    var <- paste("feat0", i, sep="")
  } else {
    var <- paste("feat", i, sep="")
  }
  feat_variables <- append(feat_variables, var)
}

# define ordered_categorical
ordered_categorical = c("checking_status", "employment", "job", "savings_status")

# define boolean variables
boolean_variables <- setdiff(colnames(df.train), c(numeric_variables, feat_variables, ordered_categorical))

# check if the division was made properly
print_unique_values(df.train[,boolean_variables])
```

### Histograms of numeric variables

```{r}
par(mfrow=c(3, 4))

for (var in numeric_variables) {
    hist(df.train[[var]], main=paste("Histogram of", var), xlab=var)
}
```

### Histograms of feat variables
```{r}
par(mfrow=c(3, 4))

for (var in feat_variables) {
    hist(df.train[[var]], main=paste("Histogram of", var), xlab=var)
}
```

### Correlation matrix

For numeric variables and target variable.

```{r, message=FALSE}
library(corrplot)
```

```{r}
num_var <- c(numeric_variables, feat_variables, "class")
numeric_df <- df.train[, num_var]
cor_matrix <- cor(numeric_df, use="complete.obs")
corrplot(cor_matrix, method="circle")
```

### Barplots of categorical variables

```{r}
# For ordered categorical variables
par(mfrow=c(2, 2))
for (var in ordered_categorical) {
  counts <- table(df.train[[var]])
  barplot(counts, main=paste("Bar Plot of", var), xlab=var, ylab="Count")
}
```

### Boolean variables

In case of boolean variables we create table with distribution of every variable.

```{r, max.height='150px'}
# Creating a summary table for boolean variables with percentages
boolean_summary <- sapply(df.train[, boolean_variables], function(x) {
  tab <- table(factor(x, levels = c(0, 1)))
  pct <- 100 * tab / sum(tab) # Convert counts to percentage
  return(pct)
})

boolean_summary <- t(boolean_summary) # Transposing to have variables as rows and percentage columns

# Formatting the table for better readability
boolean_summary <- round(boolean_summary, 1) # Round to two decimal places

# Printing the summary table
print(boolean_summary)
```

### The distribution of target variable

As we can see below, the train and test dataframes are split proportionally, however we can observe unequal distribution - there are more than twice observations with class "1" (good).

```{r}
# Target variable distribution in df:
round(table(df$class)/length(df$class), 3)
```

```{r}
# Target variabledistribution in df.train:
round(table(df.train$class)/length(df.train$class), 3)
```

```{r}
# Target variabledistribution in df.test:
round(table(df.test$class)/length(df.test$class), 3)
```

## Feature selection with RFE

### Intro to RFE

Recursive Feature Elimination (RFE) is a feature selection method used to identify and select a subset of relevant features for model building. The method works by recursively removing features, building a model with the remaining features, and calculating model accuracy. The least important features are removed in each iteration until the specified number of features is reached.

In this section of the project, we employ RFE to optimize the feature set for predicting the class variable in our dataset. The process is outlined as follows.

First of all we clear the environment, load train and test df and new libraries.

```{r, message=FALSE}
rm(list=ls())
library(mlbench)
library(caret)
library(recipes)
library(doParallel)
```

```{r}
df.train <- read.csv("./data/output/df.train.csv")
df.test <- read.csv("./data/output/df.test.csv")

# rfe requires factor as target variable, so we modify class (3rd column) - only on df.train dataframe as of now
df.train[, 1] <- as.factor(df.train[,1])
```

Detect the number of CPU cores available on the current machine and assign it to 'cores'. Then, register the detected number of cores for parallel processing using the doParallel package. This enables parallel execution of code to improve performance, especially for operations that can be executed concurrently.

```{r}
cores=detectCores()
registerDoParallel(cores=cores)
```

### Define seeds for rfeControl

If we use doParallel package and want to make our code reproducible we should pass seeds argument to rfeControl function it requires a list where each element corresponds to a specific resampling iteration's seed and the last element is a seed for the overall process.

Assuming 10-fold CV, we need a list of 11 elements: 10 for each fold and 1 for the overall process.

```{r}
set.seed(123456789)
numFolds <- 10
seedsList <- lapply(1:numFolds, function(x) sample.int(1000, 49)) # Generate random seeds for each fold

set.seed(123456789)
seedsList[[length(seedsList) + 1]] <- sample.int(1000, 1) # Add one more seed for the overall process
```


```{r}
## Define rfeControl object
control <- rfeControl(functions=rfFuncs, method="cv", number=10, seeds = seedsList)
```

### Perform feature selection with rfe
```{r}
set.seed(123456789)
results <- rfe(df.train[, seq(2, length(df.train), 1)],             # predictor variables 
               df.train[, 1],                                       # target variable for prediction
               sizes=c(1:49),                                       # the subset sizes to evaluate
               rfeControl=control)

```

Basing on the RFE method 35 variables have been chosen as the best predictors. Below we can see them.

```{r, max.height='150px'}
predictors(results)
```
Below we can see the plot of accuracy results vs number of variables included. Interesting is the fact that having more predictors than 35 not only does not increase precision but also worsens it.

```{r}
plot(results, type=c("g", "o"))
```

Assign the final columns to list and write as .csv.

```{r}
rfe_columns <- predictors(results)
write.csv(rfe_columns, "./data/output/rfe_columns.csv")
```











